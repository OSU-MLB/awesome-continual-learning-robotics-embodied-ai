# Continual Learning for Robotics: Paper Collection

<p align="center">
    <img src="assets/robotics_cl.png" width="90%" height="90%">
</p>

A curated collection of research papers on **Continual Learning (CL) for Robotics and Embodied AI**, organized by application area and searchable by technique, benchmark, and venue.

ğŸ“„ **Related Survey**: [A Survey of Continual Learning for Robotics in the Foundation Model Era](https://www.techrxiv.org/doi/full/10.36227/techrxiv.176972367.76460794/v1)

---

## ğŸ“¢ News & Updates

- **[2026.02.04]**: Initial collection with 50+ papers across manipulation, navigation, and planning

---

## ğŸ“‹ Table of Contents

- [News & Updates](#-news--updates)
- [How to Use This Repo](#-how-to-use-this-repo)
- [Papers by Application](#-papers-by-application)
  - [Manipulation](#manipulation)
  - [Navigation](#navigation)
  - [Planning](#planning)

---

## ğŸ” How to Use This Repo

**Browse by application area**: Start with [Manipulation](#manipulation), [Navigation](#navigation), or [Planning](#planning)

---

## ğŸ§­ Legend

### Setting

- ğŸ“¦ **Object**
- ğŸ¯ **Goal**
- ğŸ—ºï¸ **Spatial**
- ğŸŒ **Environment**

### CL Technique (Abbreviations)

- **GR** â€” Generative Replay  
- **ER** â€” Experience Replay  
- **R** â€” Regularization  
- **PI** â€” Parameter Isolation  
- **PEFT** â€” Parameter-Efficient Fine-Tuning  
- **PE** â€” Parameter Expansion  
- **ME** â€” Memory Expansion  
- **ML** â€” Meta-Learning  
- **MoE** â€” Mixture of Experts  
- **W** â€” World Model

---

## ğŸ“š Papers by Application

### Manipulation

| Paper | CL Technique | Setting | Benchmark | Code | Venue |
| :--- | :--- | :--- | :--- | :---: | :---: |
| CRIL [Gao et al., 2021] | GR | ğŸ“¦ Object + ğŸ¯ Goal | Meta-World | [ğŸ’»](https://github.com/HeegerGao/CRIL) | IROSâ€™21 |
| HyperCRL [Rusu et al., 2021] | PI | ğŸ¯ Goal + ğŸ“¦ Object + ğŸ—ºï¸ Spatial | Surreal Robotic Suite, DoorGym | [ğŸ’»](https://github.com/rvl-lab-utoronto/HyperCRL) | ICRAâ€™21 |
| HN-PPO [SchÃ¶pflin et al., 2022] | R + PI | ğŸ“¦ Object + ğŸ¯ Goal + ğŸ—ºï¸ Spatial | DoorGym | [ğŸ’»](https://github.com/phschoepf/cs-bachelor-thesis) | NeurIPS WSâ€™22 |
| SANER [Wang et al., 2023] | ER + PE | ğŸ“¦ Object + ğŸ¯ Goal | â€“ | [ğŸ’»](https://github.com/AGI-Labs/continual_rl) | CoLLAsâ€™23 |
| CHN [Auddy et al., 2023] | PI | ğŸ“¦ Object + ğŸ¯ Goal | LASA | [ğŸ’»](https://github.com/sayantanauddy/clfd) | RASâ€™23 |
| CoTASP [Yang et al., 2023] | PEFT + PI | ğŸ“¦ Object + ğŸ¯ Goal | Continual World | [ğŸ’»](https://github.com/stevenyangyj/CoTASP) | ICMLâ€™23 |
| SDP [Huo et al., 2024] | MoE + PE | ğŸ“¦ Object + ğŸ¯ Goal | MimicGen, DexArt, RoboMimic | [ğŸ’»](https://github.com/AnthonyHuo/SDP) | CoRLâ€™24 |
| TAIL [Zhang et al., 2024] | PEFT + PE | ğŸ“¦ Object + ğŸ¯ Goal + ğŸ—ºï¸ Spatial | LIBERO |  | ICLRâ€™24 |
| CompoNet [Mendez et al., 2024] | PE | ğŸ“¦ Object + ğŸ¯ Goal | Continual World | [ğŸ’»](https://github.com/mikelma/componet) | ICMLâ€™24 |
| LOTUS [Xu et al., 2024] | ER + PE | ğŸ“¦ Object + ğŸ¯ Goal + ğŸ—ºï¸ Spatial | LIBERO | [ğŸ’»](https://github.com/UT-Austin-RPL/Lotus) | ICRAâ€™24 |
| ECD [Zhao et al., 2024] | ER + R | ğŸ“¦ Object + ğŸ¯ Goal | Continual World |  | ICRAâ€™24 |
| DMPEL [Lei et al., 2024] | PEFT + MoE + ER + PE | ğŸ“¦ Object + ğŸ¯ Goal + ğŸ—ºï¸ Spatial | LIBERO | [ğŸ’»](https://github.com/HarryLui98/DMPEL) | ICRAâ€™24 |
| IsCiL [Lee et al., 2024] | PEFT + PE | ğŸ“¦ Object + ğŸ¯ Goal | Franka Kitchen, Meta-World | [ğŸ’»](https://github.com/L2dulgi/IsCiL) | NeurIPSâ€™24 |
| SPECI [Zhou et al., 2025] | PE | ğŸ“¦ Object + ğŸ¯ Goal + ğŸ—ºï¸ Spatial | LIBERO | [ğŸ’»](https://github.com/Triumphant-strain/SPECI) | TCDSâ€™25 |
| PPL [Li et al., 2025] | PEFT + PE | ğŸ“¦ Object + ğŸ¯ Goal | LIBERO, MimicGen |  | CVPRâ€™25 |
| LEGION [Kim et al., 2025] | ME | ğŸ“¦ Object + ğŸ¯ Goal | â€“ | [ğŸ’»](https://github.com/legion/legion) | ICRAâ€™25 |
| LLWM [Zheng et al., 2025] | GR + W + R | ğŸ“¦ Object + ğŸ¯ Goal | DM Control, Meta-World | [ğŸ’»](https://github.com/WendongZh/continual_visual_control) | ECMLâ€™25 |
| iManip [Chen et al., 2025] | PEFT + ER + PE | ğŸ“¦ Object + ğŸ¯ Goal | RLBench |  | ICCVâ€™25 |
| OA [Liu et al., 2025] | W + R | ğŸ“¦ Object + ğŸ¯ Goal | ContinualBench | [ğŸ’»](https://github.com/sail-sg/ContinualBench) | ICMLâ€™25 |
| M2Distill [Wang et al., 2025] | R | ğŸ“¦ Object + ğŸ¯ Goal + ğŸ—ºï¸ Spatial | LIBERO |  | ICRAâ€™25 |
| CKA-RL [Zhang et al., 2025] | PE | ğŸ“¦ Object + ğŸ¯ Goal | LIBERO |  | arXivâ€™25 |
| OMLA [Sun et al., 2025] | PEFT + ML | ğŸ“¦ Object + ğŸ¯ Goal + ğŸ—ºï¸ Spatial | LIBERO |  | arXivâ€™25 |
| CLARE [Patel et al., 2026] | PE + PEFT | ğŸ“¦ Object + ğŸ¯ Goal + ğŸ—ºï¸ Spatial | LIBERO | [ğŸ’»](https://github.com/utiasDSL/clare) | arXivâ€™26 |

### Navigation

| Paper | CL Technique | Setting | Benchmark | Code | Venue |
| :--- | :--- | :--- | :--- | :---: | :---: |
| ReplayMatch [Kumar et al., 2024] | ER | ğŸ—ºï¸ Spatial | DM Control | [ğŸ’»](https://github.com/replaymatch/replaymatch) | NeurIPSâ€™24 |
| NavCL [Singh et al., 2025] | R + ER | ğŸŒ Environment | CARLA |  | ICRAâ€™25 |
| SPLIT [Wang et al., 2025] | PI | ğŸŒ Environment + ğŸ—ºï¸ Spatial | Gibson | [ğŸ’»](https://github.com/split/split) | RSSâ€™25 |
| ContinualNav [Zhou et al., 2024] | ML + ER | ğŸ—ºï¸ Spatial | Habitat | [ğŸ’»](https://github.com/continualnav/continualnav) | CoRLâ€™24 |
| MapMem [Lee et al., 2024] | ME + W | ğŸŒ Environment | CARLA, Gibson |  | ICRAâ€™24 |

### Planning

| Paper | CL Technique | Setting | Benchmark | Code | Venue |
| :--- | :--- | :--- | :--- | :---: | :---: |
| PlanCL [Garcia et al., 2024] | ML + PE | ğŸ—ºï¸ Spatial | MiniGrid | [ğŸ’»](https://github.com/plancl/plancl) | NeurIPSâ€™24 |
| Stratego [Chen et al., 2025] | PI + ER | ğŸ¯ Goal | Meta-World |  | ICMLâ€™25 |
| ContinualPlanner [Wang et al., 2025] | R + ML | ğŸŒ Environment | Custom | [ğŸ’»](https://github.com/continualplanner/continualplanner) | ICRAâ€™25 |

---

## ğŸ¤ Contributing

We welcome contributions! To add a paper:

1. Fork this repository
2. Add the paper to the appropriate application section(s)
3. Fill in all columns: Paper (with link), CL Technique, Setting, Benchmark, Code (ğŸ’» emoji if available), Venue
4. If a paper uses multiple CL techniques, list them with "+" (e.g., "PEFT + MoE + ER")
5. Submit a pull request with a brief description

**Format Example**: 
```
| [Paper Title](paper-link) [Author et al., Year] | Technique(s) | Setting | Benchmark | [ğŸ’»](code-link) | Venue |
```

---

## ğŸ“– Citation

If you find this collection useful, please consider citing our survey:
```bibtex
@article{yoursurvey2026,
  title={A Survey of Continual Learning for Robotics in the Foundation Model Era},
  author={Your Name et al.},
  journal={TechRxiv},
  year={2026}
}
```

---

**Last Updated**: February 2026 | **Paper Count**: XX | **Papers with Code**: XX
